{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db1cd56-d756-4188-a28b-3ea37641aef6",
   "metadata": {},
   "source": [
    "# Network Traffic Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3ee52-950c-4171-85bd-4616d4ffb690",
   "metadata": {},
   "source": [
    "# Dataset Description: \n",
    "An extensive **IoT attack** dataset. Data consists of 33 Attacks which were executed in an IoT topology composed of 105 devices. These attacks are classified into seven categories, namely **DDoS, DoS, Recon, Web-based, Brute Force, Spoofing, and Mirai**. All attacks were executed by malicious IoT devices targeting other IoT devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28936ef-f041-4f6c-a622-b3f3e41f5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first, some general imports:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ddca33-a492-42f2-bd95-50bc92e6a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Note that the data comes in a set of 169 csv files of the naming convention \"part-00xxx-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\" where xxx ranges from 000 to 168\n",
    "# So we must load and merge all of the csv files into one large dataset\n",
    "\n",
    "# Loop through each csv file in the directory, read into pandas dataframe and append it to the list of sub-datasets\n",
    "dataset_path = '../data/CICIot2023/'\n",
    "df_sets = [k for k in os.listdir(dataset_path) if k.endswith('.csv')]\n",
    "df_sets.sort()\n",
    "training_sets = df_sets[:int(len(df_sets)*.8)]\n",
    "test_sets = df_sets[int(len(df_sets)*.2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b6b45-54a4-4d7e-9be2-ae1460975194",
   "metadata": {},
   "source": [
    "## First, we will employ a lightweight \"screening\" classifier which categorizes traffic as either benign of malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d58bcc-2591-430e-851d-13f409d1f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "       'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "       'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "       'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "       'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "       'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1c2ded-152e-4e10-beae-f9decf38049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85854e0c-3a75-47d9-b571-44e25cace4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████▋                                             | 41/135 [00:28<01:14,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "for train_set in tqdm(training_sets):\n",
    "    scaler.fit(pd.read_csv(dataset_path + train_set)[X_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5499c-fb72-4e05-a690-d66eed022e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a72fd8-3bb4-4e79-9df6-3eea8501dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Multinomial Naive Bayes\",\n",
    "    \"Bernoulli Naive Bayes\",\n",
    "    \"Perceptron\",\n",
    "    \"SGDClassifier\",\n",
    "    \"PassiveAggressiveClassifier\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    Perceptron(tol=1e3, random_state=0),\n",
    "    SGDClassifier(),\n",
    "    PassiveAggressiveClassifier(max_iter=2000, random_state=0, tol=1e3)\n",
    "]\n",
    "\n",
    "for model in (classifiers):\n",
    "    # Introduce first chunk flag\n",
    "    first_chunk=True\n",
    "    for train_set in tqdm(training_sets):\n",
    "        # We need to implement incremental learning due to dataset being *much* larger than RAM\n",
    "        # Read data into chunks\n",
    "        reader = pd.read_csv(dataset_path + train_set, iterator=True, chunksize=300000)\n",
    "        #d[X_columns] = scaler.transform(d[X_columns]\n",
    "        # Incremental Learning\n",
    "        for chunk in reader:\n",
    "            X = chunk[X_columns]\n",
    "            y = chunk[y_column]\n",
    "\n",
    "            if first_chunk:\n",
    "                # Pass the classes parameter during the first call to partial_fit()\n",
    "                model.partial_fit(X, y, classes=np.unique(y))\n",
    "                first_chunk=False\n",
    "            else:\n",
    "                model.partial_fit(X, y)\n",
    "        del reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf236d-fe02-49cc-b6b6-3ba1ddeab177",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "preds = {i:[] for i in range(len(names))}\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    model = classifiers[i]\n",
    "    for test_set in tqdm(test_sets):\n",
    "        d_test = pd.read_csv(dataset_path + test_set)\n",
    "        d_test[X_columns] = scaler.transform(d_test[X_columns])\n",
    "        y_test += list(d_test[y_column].values)\n",
    "        y_pred = list(model.predict(d_test[X_columns]))\n",
    "        preds[i] = preds[i] + y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2853a70-5b1b-4531-8ebe-90e5f14d013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "for k,v in preds.items():\n",
    "    y_pred = v\n",
    "    print(f\"##### {Model_names[k]} (34 classes) #####\")\n",
    "    print('accuracy_score: ', accuracy_score(y_pred, y_test))\n",
    "    print('recall_score: ', recall_score(y_pred, y_test, average='macro'))\n",
    "    print('precision_score: ', precision_score(y_pred, y_test, average='macro'))\n",
    "    print('f1_score: ', f1_score(y_pred, y_test, average='macro'))\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15e3bd-41b8-4a3a-9b80-4f0c77260682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Ensemble technique: Max Voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble1 = VotingClassifier(estimators=[('MNB', classifiers[0]), ('BNB', classifiers[1]), ('Per', classifiers[2]), ('SGDC', classifiers[3]), ('PAC', classifiers[4])], voting='hard')\n",
    "\n",
    "# Train Ensemble Model\n",
    "\n",
    "# Introduce first chunk flag\n",
    "first_chunk=True\n",
    "for train_set in tqdm(training_sets):\n",
    "    # We need to implement incremental learning due to dataset being *much* larger than RAM\n",
    "    # Read data into chunks\n",
    "    reader = pd.read_csv(dataset_path + train_set, iterator=True, chunksize=300000)\n",
    "    # Incremental Learning\n",
    "    for chunk in reader:\n",
    "        X = chunk[X_columns]\n",
    "        y = chunk[y_column]\n",
    "\n",
    "        if first_chunk:\n",
    "            # Pass the classes parameter during the first call to partial_fit()\n",
    "            model.partial_fit(X, y, classes=np.unique(y))\n",
    "            first_chunk=False\n",
    "        else:\n",
    "            model.partial_fit(X, y)\n",
    "    del reader\n",
    "\n",
    "# Test Ensemble Model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
